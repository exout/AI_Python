{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c9d9d7-8c1b-4e89-a4f9-ba143382644b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')\n",
    "nltk.download('movie_reviews')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbcafa62-75a8-4381-af8c-404273cc9f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords, movie_reviews\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f1b4a7b-aebf-4ed4-93aa-68c3ecf327c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This data has (2000, 2) Dimensions.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  sentiment                                             review\n0       neg  plot : two teen couples go to a church party ,...\n1       neg  the happy bastard's quick movie review \\ndamn ...\n2       neg  it is movies like these that make a jaded movi...\n3       neg   \" quest for camelot \" is warner bros . ' firs...\n4       neg  synopsis : a mentally unstable man undergoing ...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sentiment</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>neg</td>\n      <td>plot : two teen couples go to a church party ,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>neg</td>\n      <td>the happy bastard's quick movie review \\ndamn ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>neg</td>\n      <td>it is movies like these that make a jaded movi...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>neg</td>\n      <td>\" quest for camelot \" is warner bros . ' firs...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>neg</td>\n      <td>synopsis : a mentally unstable man undergoing ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = []\n",
    "for fileid in movie_reviews.fileids():\n",
    "    tag, filename = fileid.split(\"/\")\n",
    "    reviews.append((tag, movie_reviews.raw(fileid)))\n",
    "df = pd.DataFrame(reviews, columns=['sentiment', 'review'])\n",
    "print(f'This data has {df.shape} Dimensions.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ec7f484-45d6-4fe6-b7ec-94096c7fd240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0    1000\n1    1000\nName: sentiment, dtype: int64"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# här ändrar vi 'pos' till 1 och 'neg' till 0\n",
    "df['sentiment'] = np.where(df['sentiment'] == 'pos', 1, 0)\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7223c278-1f2c-412d-b162-86155b4662d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dimensions: ((1400,), (1400,))\n",
      "The test dimensions: ((600,), (600,))\n",
      "0    700\n",
      "1    700\n",
      "Name: sentiment, dtype: int64\n",
      "0    300\n",
      "1    300\n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train , y_test = train_test_split(df['review'], \n",
    "df['sentiment'],test_size=0.3, random_state=123)\n",
    "\n",
    "print(f'The train dimensions: {X_train.shape, y_train.shape}')\n",
    "print(f'The test dimensions: {X_test.shape, y_test.shape}')\n",
    "\n",
    "# kolla in fördelningen mellan tranings och test data.\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf2d647-20e1-435c-a86d-99b6d6eadde5",
   "metadata": {},
   "source": [
    "### pre processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42c5c012-f774-4169-b472-916624d56893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1400, 27676)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pre_processing(data):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') # tokenize words medan ignorerar punctuation\n",
    "    tokens = tokenizer.tokenize(data)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer() # lower och lemma\n",
    "    lemmas = [lemmatizer.lemmatize(token.lower(), pos='v')\n",
    "             for token in tokens]\n",
    "    \n",
    "    # tabort stop_words\n",
    "    stop_words = [lemma for lemma in lemmas if lemma not in \n",
    "                  stopwords.words('english')]\n",
    "    return stop_words\n",
    "\n",
    "vectoriser = TfidfVectorizer(analyzer=pre_processing)\n",
    "# fit och transform \n",
    "X_train_tf_idf = vectoriser.fit_transform(X_train)\n",
    "X_train_tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3017431-e18f-4a56-a5b8-385511ac039e",
   "metadata": {},
   "source": [
    "### Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "696eeedf-da5d-4b27-89a7-22f15cf4ffd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Scores:[0.82857143 0.85       0.84285714 0.81785714 0.81428571]\n",
      "Accuracy: 0.831 (0.028)\n"
     ]
    }
   ],
   "source": [
    "sgd_clf = SGDClassifier(random_state=123)\n",
    "sgd_clf_Scores = cross_val_score(sgd_clf, X_train_tf_idf, y_train, cv=5)\n",
    "\n",
    "print(f'SGD Scores:{sgd_clf_Scores}')\n",
    "print(\"Accuracy: %0.3f (%0.3f)\" % (sgd_clf_Scores.mean(),\n",
    "                                       sgd_clf_Scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4189a8c5-8b5a-4820-aab2-dd947f73b4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([0.82857143, 0.85      , 0.84285714, 0.81785714, 0.81428571])"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(sgd_clf, X_train_tf_idf, y_train, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd98e773-dd4f-4339-baac-f9c565d475a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[580 120]\n",
      " [117 583]]\n"
     ]
    }
   ],
   "source": [
    "sgd_clf_pred = cross_val_predict(sgd_clf, X_train_tf_idf, y_train, cv=5)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_train, sgd_clf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da9a9d8-fe7c-4472-bc57-4f288e438d08",
   "metadata": {},
   "source": [
    "### hitta den bästa algoritmen for att kontrollera lärningsprocessen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6def378-9b39-4805-9a04-0c5004771ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'early_stopping': False,\n 'fit_intercept': False,\n 'loss': 'log',\n 'penalty': 'l1'}"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = {'fit_intercept': [True,False],\n",
    "        'early_stopping': [True, False],\n",
    "        'loss' : ['hinge', 'log', 'squared_hinge'],\n",
    "        'penalty' : ['l2', 'l1', 'none']}\n",
    "search = GridSearchCV(estimator=sgd_clf, param_grid=lr, cv=5)\n",
    "search.fit(X_train_tf_idf, y_train)\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa3ffd94-36eb-4112-a9b3-85a5a662ffbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85       0.85714286 0.83571429 0.84285714 0.82857143]\n",
      "Accuracy: 0.84 (0.02)\n"
     ]
    }
   ],
   "source": [
    "grid_sgd_clf_scores = cross_val_score(search.best_estimator_,\n",
    "X_train_tf_idf, y_train, cv=5)\n",
    "print(grid_sgd_clf_scores)\n",
    "print('Accuracy: %0.2f (%0.2f)' % (grid_sgd_clf_scores.mean(),\n",
    "                                       grid_sgd_clf_scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a45457-3842-4293-8bb6-4bc2159ff4b1",
   "metadata": {},
   "source": [
    "### pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61d11b59-2ec1-4746-b023-e589119463de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Pipeline(steps=[('Vectorizer',\n                 TfidfVectorizer(analyzer=<function pre_processing at 0x0000024E67898160>)),\n                ('Classifier',\n                 SGDClassifier(fit_intercept=False, loss='log', penalty='l1',\n                               random_state=123))])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Pipeline([('Vectorizer', vectoriser),\n",
    "                ('Classifier', search.best_estimator_)])\n",
    "\n",
    "p.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d027073e-f5e9-41f0-b441-39a90cd113fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.853\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = p.predict(X_test)\n",
    "print(\"Accuracy: %0.3f\" % (accuracy_score(y_test, y_test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f012297f-6910-4c75-bca1-116caa708db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[249  51]\n",
      " [ 37 263]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_test_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-aba6cf2f",
   "language": "python",
   "display_name": "PyCharm (sentiments_analysis_examples)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}